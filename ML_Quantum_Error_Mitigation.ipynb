{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Quantum Error Mitigation\n",
    "\n",
    "Welcome! This notebook demonstrates how to learn corrections for noisy quantum expectation values using Graph Neural Networks (GNNs). It will walk you through the full data-generation ‚Üí modeling ‚Üí training ‚Üí evaluation workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you'll learn\n",
    "- **Quantum noise modeling** with a realistic (QuEra-inspired) error model\n",
    "- **Circuit-to-graph conversion** for GNN inputs\n",
    "- **A GATv2-based model** that predicts a correction to noisy expectations\n",
    "- **Training & evaluation** on simulated datasets\n",
    "- **Interpolation vs extrapolation** across different noise levels\n",
    "- **Model visualization** (architecture diagram)\n",
    "\n",
    "### Notebook structure\n",
    "1. Setup & Noise Model\n",
    "2. Model and Feature Engineering (circuit graphs, observable features)\n",
    "3. Data Generation & Simulation (noisy vs noiseless expectations)\n",
    "4. Train/Validation Split & Training Loop\n",
    "5. Evaluation & Visualizations\n",
    "6. Interpolation/Extrapolation across Noise Factors\n",
    "7. Model Architecture Visualization (torchviz + schematic)\n",
    "8. Possible Research Directions \n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Noise Model\n",
    "\n",
    "This section imports dependencies and defines a realistic noise model based on literature-reported error rates (Bluvstein et al., Nature 626, 2024). We'll reuse it for generating noisy expectation values.\n",
    "\n",
    "- **Reset, measurement, single- and two-qubit errors** are modeled with Pauli channels.\n",
    "- A **scaling factor** allows us to sweep noise strength later for interpolation/extrapolation experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model and Feature Engineering\n",
    "\n",
    "We encode circuits as graphs via `circuit_to_graph_data`:\n",
    "- **Nodes**: operations plus input/output placeholders\n",
    "- **Node features**: one-hot gate type, I/O markers, normalized qubit index\n",
    "- **Edges**: DAG successor links\n",
    "\n",
    "The model `QErrorMitigationModel` uses GATv2 layers to produce a circuit embedding and fuses it with:\n",
    "- **Observable features** (simple structural stats)\n",
    "- **Noise factor** (scalar)\n",
    "- **Noisy expectation** (scalar)\n",
    "\n",
    "It predicts a **correction** that, when added to the noisy value, approximates the clean expectation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Generation & Simulation\n",
    "\n",
    "We generate a diverse set of circuits (`random`, `EfficientSU2`, and `UCCSD-like`) and pair them with random Pauli-sum observables. For each noise factor:\n",
    "- Run statevector (noiseless) and density-matrix (noisy) estimations\n",
    "- Store noisy and clean expectation values\n",
    "- Visualize noisy vs clean scatter plots and summary statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Validation Split & Training Loop\n",
    "\n",
    "We prepare the dataset for the GNN:\n",
    "- Convert circuits to graphs and observables to features\n",
    "- Split into train/validation/test\n",
    "- Train with MSE loss on corrected expectations (noisy + predicted correction vs true)\n",
    "- Track learning rate, time per epoch, and losses; save best model checkpoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Visualizations\n",
    "\n",
    "We evaluate on a held-out test set and produce:\n",
    "- **Scatter plots** of predicted vs true and noisy vs true values\n",
    "- **Error histograms** and correction distributions\n",
    "- **JSON dumps** for downstream analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpolation/Extrapolation across Noise Factors\n",
    "\n",
    "We reuse the trained model on datasets generated with different noise factors to study:\n",
    "- **Interpolation** performance within the training noise range\n",
    "- **Extrapolation** performance outside the training range\n",
    "\n",
    "We summarize with error vs noise-factor plots and error-reduction curves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Architecture Visualization\n",
    "\n",
    "We provide two visualizations:\n",
    "- **Automatic graph** via `torchviz.make_dot`\n",
    "- **Schematic** using `networkx` to highlight data paths (circuit, observable, noise, fusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. GPU4Quantum Challenge: Your Mission üéØ\n",
    "\n",
    "Here are exciting research directions for the GPU4Quantum challenge:\n",
    "\n",
    "## üèÜ High-Impact Research Areas\n",
    "\n",
    "### 1. Scalable Data Generation\n",
    "**Challenge**: Generate training data for 50+ qubit circuits\n",
    "- **Current bottleneck**: Classical simulation becomes exponentially hard\n",
    "- **GPU opportunity**: Use tensor network methods, approximate simulation\n",
    "- **Expected impact**: Enable training on realistic problem sizes\n",
    "\n",
    "### 2. Advanced GNN Architectures\n",
    "**Challenge**: Design quantum-aware neural architectures\n",
    "- **Ideas**: Quantum attention mechanisms, physics-informed losses\n",
    "- **GPU opportunity**: Experiment with large-scale architectures\n",
    "- **Expected impact**: Better generalization across quantum devices\n",
    "\n",
    "### 3. Transfer Learning Across Quantum Hardware\n",
    "**Challenge**: Pre-train on simulated data, adapt to real hardware\n",
    "- **GPU opportunity**: Large-scale pre-training, efficient fine-tuning\n",
    "- **Expected impact**: Reduce calibration time for new quantum devices\n",
    "\n",
    "### 4. Multi-task Learning\n",
    "**Challenge**: Learn multiple error mitigation strategies jointly\n",
    "- **Ideas**: Predict multiple observables, different noise levels\n",
    "- **GPU opportunity**: Large multi-task architectures\n",
    "- **Expected impact**: More robust and versatile error mitigation\n",
    "\n",
    "## üõ†Ô∏è Technical Deep Dives\n",
    "\n",
    "### GPU-Accelerated Quantum Simulation\n",
    "```python\n",
    "# Pseudocode for GPU-accelerated data generation\n",
    "import cupy as cp  # or use cuQuantum\n",
    "\n",
    "def gpu_batch_simulation(circuits, noise_models, observables):\n",
    "    # Batch process thousands of circuits in parallel\n",
    "    batch_states = cp.zeros((len(circuits), 2**max_qubits), dtype=cp.complex64)\n",
    "    \n",
    "    # Parallel gate application\n",
    "    for gate_layer in circuit_layers:\n",
    "        batch_states = apply_gates_gpu(batch_states, gate_layer)\n",
    "        batch_states = apply_noise_gpu(batch_states, noise_models)\n",
    "    \n",
    "    # Parallel expectation value computation\n",
    "    expectations = compute_expectations_gpu(batch_states, observables)\n",
    "    return expectations\n",
    "```\n",
    "\n",
    "### Advanced GNN Components\n",
    "```python\n",
    "# Quantum-aware attention mechanism\n",
    "class QuantumAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.quantum_weights = nn.Parameter(torch.randn(hidden_dim))\n",
    "        \n",
    "    def forward(self, node_features, quantum_properties):\n",
    "        # Attention weights based on quantum properties\n",
    "        # (entanglement, gate fidelities, etc.)\n",
    "        attention = torch.softmax(\n",
    "            torch.matmul(node_features, self.quantum_weights) * quantum_properties,\n",
    "            dim=-1\n",
    "        )\n",
    "        return attention\n",
    "```\n",
    "\n",
    "## üìä Evaluation Metrics & Benchmarks\n",
    "\n",
    "Success in GPU4Quantum can be measured by:\n",
    "\n",
    "1. **Error Reduction**: % improvement in expectation value accuracy\n",
    "2. **Scalability**: Largest circuit size handled (qubits √ó depth)\n",
    "3. **Speed**: Training time, inference time\n",
    "4. **Generalization**: Performance across different:\n",
    "   - Circuit types (optimization, simulation, ML)\n",
    "   - Noise models (hardware-specific)\n",
    "   - Observable types (local, global, multi-qubit)\n",
    "\n",
    "\n",
    "## üåü Getting Started\n",
    "\n",
    "1. **Fork this repository** and set up your development environment\n",
    "2. **Start small**: Reproduce the results in this notebook\n",
    "3. **Scale up**: Use GPU4Quantum resources to train larger models\n",
    "4. **Innovate**: Implement your own architectural improvements\n",
    "5. **Share**: Publish your findings and contribute back to the community\n",
    "\n",
    "The future of quantum computing depends on solving the noise problem. With GPU4Quantum resources, you have the power to make a real impact! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_aer.noise import (\n",
    "    NoiseModel,\n",
    "    pauli_error,\n",
    ")\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union, cast\n",
    "\n",
    "\n",
    "def get_quera_noise_model(config_quera_noise_factor=1.0):\n",
    "    \"\"\"\n",
    "    Constructs a quantum noise model based on error parameters\n",
    "    derived from the Bluvstein et al., Nature Vol 626, (2024) paper.\n",
    "\n",
    "    Args:\n",
    "        config_quera_noise_factor (float): A factor to scale the error probabilities.\n",
    "                                           Default is 1.0 (no scaling).\n",
    "\n",
    "    Returns:\n",
    "        NoiseModel: The Qiskit Aer noise model.\n",
    "    \"\"\"\n",
    "    quera_noise_model = NoiseModel()\n",
    "\n",
    "    # 1. Reset error\n",
    "    # Supported by SPAM error (Ref. [8] cited by paper) and measurement fidelity (paper p2, p10).\n",
    "    # e.g., SPAM ~0.006, Meas_Error ~0.002 => Reset_Error ~0.004\n",
    "    p_reset = 0.004 * config_quera_noise_factor\n",
    "    error_reset = pauli_error([(\"X\", p_reset), (\"I\", 1 - p_reset)])\n",
    "    quera_noise_model.add_all_qubit_quantum_error(error_reset, \"reset\")\n",
    "\n",
    "    # 2. Measurement error\n",
    "    # Paper (p2, Fig 1c; p10) mentions ~99.8% readout fidelity (0.002 error).\n",
    "    # SPAM data from Ref. [8] also supports p_meas around 0.003.\n",
    "    p_meas = 0.003 * config_quera_noise_factor\n",
    "    error_meas = pauli_error([(\"X\", p_meas), (\"I\", 1 - p_meas)])\n",
    "    quera_noise_model.add_all_qubit_quantum_error(error_meas, \"measure\")\n",
    "\n",
    "    # 3. Entangling errors only for the active qubits in the CZ gate\n",
    "    # Paper (p4, p9) states two-qubit gate fidelity of 99.5% (0.005 error).\n",
    "    p_cz_active_qub = 0.005 * config_quera_noise_factor\n",
    "    # The specific Pauli distribution X(p/4), Y(p/4), Z(p/2) is a modeling choice.\n",
    "    cz_single_qubit_error = pauli_error(\n",
    "        [\n",
    "            (\"X\", 1 / 4 * p_cz_active_qub),\n",
    "            (\"Y\", 1 / 4 * p_cz_active_qub),\n",
    "            (\"Z\", 1 / 2 * p_cz_active_qub),\n",
    "            (\"I\", 1 - p_cz_active_qub),\n",
    "        ]\n",
    "    )\n",
    "    cz_error = cz_single_qubit_error.tensor(cz_single_qubit_error)\n",
    "    quera_noise_model.add_quantum_error(\n",
    "        cz_error, [\"cx\", \"ecr\", \"cz\"], [0,1] # Apply to specific qubits for 2Q gates\n",
    "    )\n",
    "    # To apply to all instances of 2Q gates if qubits are not specified\n",
    "    # quera_noise_model.add_all_qubit_quantum_error(cz_error, [\"cx\", \"ecr\", \"cz\"]) # This is less precise for 2Q gates\n",
    "\n",
    "    # 4. Single-qubit gate errors\n",
    "    # p_u1: Supported by Ref. [8] (avg 1Q fidelity 99.94%, error 6e-4) and\n",
    "    #       paper (p10, Raman scattering limit error ~7e-4).\n",
    "    p_u1 = 5e-4 * config_quera_noise_factor\n",
    "    # p_u2: Supported by paper (p10, local Z(pi/2) fidelity 99.912%, error ~8.8e-4).\n",
    "    p_u2 = 1e-3 * config_quera_noise_factor\n",
    "    # p_u3: Plausible for more complex/longer 1Q gates, qualitatively supported\n",
    "    #       by paper's discussion of variable gate durations (p10, Ext. Data Fig. 2b).\n",
    "    p_u3 = 1.5e-3 * config_quera_noise_factor\n",
    "\n",
    "    # Depolarizing-like Pauli error model for single-qubit gates.\n",
    "    sq_error_u1 = pauli_error(\n",
    "        [\n",
    "            (\"X\", 1 / 3 * p_u1),\n",
    "            (\"Y\", 1 / 3 * p_u1),\n",
    "            (\"Z\", 1 / 3 * p_u1),\n",
    "            (\"I\", 1 - p_u1),\n",
    "        ]\n",
    "    )\n",
    "    # Apply to gates corresponding to simpler/shorter rotations\n",
    "    quera_noise_model.add_all_qubit_quantum_error(\n",
    "        sq_error_u1, [\"u1\", \"rz\", \"ry\", \"rx\", \"sx\", \"sxdg\", \"x\", \"y\", \"z\", \"h\"] # Common 1Q gates\n",
    "    )\n",
    "\n",
    "    sq_error_u2 = pauli_error(\n",
    "        [\n",
    "            (\"X\", 1 / 3 * p_u2),\n",
    "            (\"Y\", 1 / 3 * p_u2),\n",
    "            (\"Z\", 1 / 3 * p_u2),\n",
    "            (\"I\", 1 - p_u2),\n",
    "        ]\n",
    "    )\n",
    "    # Apply to gates corresponding to moderately complex rotations (e.g., U2)\n",
    "    quera_noise_model.add_all_qubit_quantum_error(sq_error_u2, [\"u2\"])\n",
    "\n",
    "    sq_error_u3 = pauli_error(\n",
    "        [\n",
    "            (\"X\", 1 / 3 * p_u3),\n",
    "            (\"Y\", 1 / 3 * p_u3),\n",
    "            (\"Z\", 1 / 3 * p_u3),\n",
    "            (\"I\", 1 - p_u3),\n",
    "        ]\n",
    "    )\n",
    "    # Apply to gates corresponding to more complex rotations (e.g., U3)\n",
    "    quera_noise_model.add_all_qubit_quantum_error(sq_error_u3, [\"u3\", \"u\"]) # u is a general 1Q gate\n",
    "\n",
    "    return quera_noise_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the model architecture\n",
    "class QErrorMitigationModel(nn.Module):\n",
    "    def __init__(self, depth=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        in_channels = 17\n",
    "\n",
    "        for i in range(depth):\n",
    "            out_channels = 128 if i < depth-1 else 32\n",
    "            self.conv_layers.append(GATv2Conv(in_channels=in_channels, out_channels=out_channels))\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        \n",
    "        # Circuit encoder (GAT layers)\n",
    "        # self.conv1 = GATv2Conv(in_channels=17, out_channels=128)  # 17 = feature dimensions from circuit_to_graph\n",
    "        # # self.conv2 = GATv2Conv(in_channels=128, out_channels=64)\n",
    "        # self.conv2 = GATv2Conv(in_channels=128, out_channels=32)\n",
    "        \n",
    "        # Observable encoder - small MLP for observable features\n",
    "        self.obs_encoder = nn.Sequential(\n",
    "            nn.Linear(5, 32),  # 5 = from extract_observable_features output size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 8)\n",
    "        )\n",
    "        \n",
    "        # Noise model encoder (can be expanded later)\n",
    "        self.noise_encoder = nn.Linear(1, 4)  # Just using noise_factor for now\n",
    "        \n",
    "        # Fusion network - now predicts the correction instead of the true value\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(32 + 8 + 4 + 1, 256),  # +1 for noisy expectation value\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)  # Output: correction to apply to noisy expectation value\n",
    "        )\n",
    "        \n",
    "    def forward(self, circuit_data, observable_features, noise_factor, noisy_exp):\n",
    "        # Process circuit graph\n",
    "        x, edge_index, batch = circuit_data.x, circuit_data.edge_index, circuit_data.batch\n",
    "        \n",
    "        for conv in self.conv_layers:\n",
    "            x = F.elu(conv(x, edge_index))\n",
    "        # # Apply GAT layers\n",
    "        # x = F.elu(self.conv1(x, edge_index))\n",
    "        # x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Global pooling to get circuit embedding\n",
    "        circuit_embedding = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Process observable features\n",
    "        obs_embedding = self.obs_encoder(observable_features)\n",
    "        \n",
    "        # Process noise information\n",
    "        noise_embedding = self.noise_encoder(noise_factor)\n",
    "        \n",
    "        # Concatenate with noisy expectation value\n",
    "        combined = torch.cat([\n",
    "            circuit_embedding, \n",
    "            obs_embedding, \n",
    "            noise_embedding, \n",
    "            noisy_exp.view(-1, 1)\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Predict correction to noisy expectation value\n",
    "        correction = self.fusion(combined)\n",
    "        \n",
    "        # Return the corrected expectation value (noisy + correction)\n",
    "        return noisy_exp + correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create dataset preparation function\n",
    "from qiskit.quantum_info.operators.base_operator import BaseOperator\n",
    "\n",
    "def extract_observable_features(observable: BaseOperator) -> List[float]:\n",
    "    \"\"\"\n",
    "    Extract features from an observable.\n",
    "    \n",
    "    Args:\n",
    "        observable: The observable to extract features from\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: The extracted features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # # Convert to SparsePauliOp if needed\n",
    "    # if isinstance(observable, PauliSumOp):\n",
    "    #     pauli_op = observable.primitive\n",
    "    if hasattr(observable, 'to_pauli_op'):\n",
    "        pauli_op = observable.to_pauli_op()\n",
    "    else:\n",
    "        # If we can't convert, return minimal features\n",
    "        features.append(1.0)  # Number of terms\n",
    "        features.append(0.0)  # Number of X terms\n",
    "        features.append(0.0)  # Number of Y terms\n",
    "        features.append(0.0)  # Number of Z terms\n",
    "        features.append(0.0)  # Average weight\n",
    "        return features\n",
    "    \n",
    "    # Number of terms\n",
    "    features.append(float(len(pauli_op)))\n",
    "    \n",
    "    # Count Pauli types\n",
    "    x_count = 0\n",
    "    y_count = 0\n",
    "    z_count = 0\n",
    "    weights = []\n",
    "    \n",
    "    for pauli_string, _ in zip(pauli_op.paulis, pauli_op.coeffs):\n",
    "        pauli_str = pauli_string.to_label()\n",
    "        x_count += pauli_str.count('X')\n",
    "        y_count += pauli_str.count('Y')\n",
    "        z_count += pauli_str.count('Z')\n",
    "        weight = len(pauli_str) - pauli_str.count('I')\n",
    "        weights.append(weight)\n",
    "    \n",
    "    features.append(float(x_count))\n",
    "    features.append(float(y_count))\n",
    "    features.append(float(z_count))\n",
    "    \n",
    "    # Average weight\n",
    "    avg_weight = np.mean(weights) if weights else 0.0\n",
    "    features.append(float(avg_weight))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_mitigator_dataset(circuits, observables, noisy_values, noiseless_values, noise_factor):\n",
    "    \"\"\"Prepare dataset for the error mitigation model\n",
    "    \n",
    "    Args:\n",
    "        circuits: List of quantum circuits or list of lists of circuits for multiple noise factors\n",
    "        observables: List of observables or list of lists of observables for multiple noise factors\n",
    "        noisy_values: List of noisy expectation values or list of lists for multiple noise factors\n",
    "        noiseless_values: List of noiseless expectation values or list of lists for multiple noise factors\n",
    "        noise_factor: Single noise factor value or list of noise factor values\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing the dataset entries\n",
    "    \"\"\"\n",
    "    # from ml_qem.features import extract_observable_features\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    # Check if we have multiple noise factors\n",
    "    if isinstance(noise_factor, (list, tuple, np.ndarray)):\n",
    "        # Process multiple noise factors\n",
    "        for nf_idx, nf in enumerate(noise_factor):\n",
    "            # Get the corresponding data for this noise factor\n",
    "            if isinstance(circuits[0], list):\n",
    "                curr_circuits = circuits[nf_idx]\n",
    "                curr_observables = observables[nf_idx]\n",
    "                curr_noisy_values = noisy_values[nf_idx]\n",
    "                curr_noiseless_values = noiseless_values[nf_idx]\n",
    "            else:\n",
    "                # If only noise_factor is a list but other inputs aren't lists of lists,\n",
    "                # use the same data for each noise factor\n",
    "                curr_circuits = circuits\n",
    "                curr_observables = observables\n",
    "                curr_noisy_values = noisy_values\n",
    "                curr_noiseless_values = noiseless_values\n",
    "            \n",
    "            # Process the data for this noise factor\n",
    "            for i in range(len(curr_circuits)):\n",
    "                circuit_graph = circuit_to_graph_data(curr_circuits[i])\n",
    "                obs_features = torch.tensor(extract_observable_features(curr_observables[i]), dtype=torch.float)\n",
    "                correction = curr_noiseless_values[i] - curr_noisy_values[i]\n",
    "                \n",
    "                dataset.append({\n",
    "                    'circuit_graph': circuit_graph,\n",
    "                    'observable_features': obs_features,\n",
    "                    'noise_factor': torch.tensor([nf], dtype=torch.float),\n",
    "                    'noisy_exp': torch.tensor([curr_noisy_values[i]], dtype=torch.float),\n",
    "                    'true_exp': torch.tensor([curr_noiseless_values[i]], dtype=torch.float),\n",
    "                    'correction': torch.tensor([correction], dtype=torch.float),\n",
    "                    'circuit': curr_circuits[i]\n",
    "                })\n",
    "    else:\n",
    "        # Original single noise factor case\n",
    "        for i in range(len(circuits)):\n",
    "            # Convert circuit to graph\n",
    "            circuit_graph = circuit_to_graph_data(circuits[i])\n",
    "            \n",
    "            # Extract observable features\n",
    "            obs_features = torch.tensor(extract_observable_features(observables[i]), dtype=torch.float)\n",
    "            \n",
    "            # Calculate the correction (true - noisy)\n",
    "            correction = noiseless_values[i] - noisy_values[i]\n",
    "            \n",
    "            # Store as dictionary\n",
    "            dataset.append({\n",
    "                'circuit_graph': circuit_graph,\n",
    "                'observable_features': obs_features,\n",
    "                'noise_factor': torch.tensor([noise_factor], dtype=torch.float),\n",
    "                'noisy_exp': torch.tensor([noisy_values[i]], dtype=torch.float),\n",
    "                'true_exp': torch.tensor([noiseless_values[i]], dtype=torch.float),\n",
    "                'correction': torch.tensor([correction], dtype=torch.float),\n",
    "                'circuit': circuits[i]\n",
    "            })\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define training function\n",
    "def train_mitigator(dataset, model, epochs=100, batch_size=32, lr=0.001, device='cpu'):\n",
    "    \"\"\"Train the error mitigation model\"\"\"\n",
    "    \n",
    "    # Split into train/validation\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    train_dataset = dataset[:train_size]\n",
    "    val_dataset = dataset[train_size:]\n",
    "    \n",
    "    # Create data loaders with custom collate function\n",
    "    def collate_fn(batch):\n",
    "        # Collate circuit graphs\n",
    "        graphs = [item['circuit_graph'] for item in batch]\n",
    "        circuit_batch = Batch.from_data_list(graphs)\n",
    "        \n",
    "        # Collate other features\n",
    "        obs_batch = torch.stack([item['observable_features'] for item in batch])\n",
    "        noise_batch = torch.stack([item['noise_factor'] for item in batch])\n",
    "        noisy_exp_batch = torch.stack([item['noisy_exp'] for item in batch])\n",
    "        true_exp_batch = torch.stack([item['true_exp'] for item in batch])\n",
    "        correction_batch = torch.stack([item['correction'] for item in batch])\n",
    "        \n",
    "        return {\n",
    "            'circuit_data': circuit_batch,\n",
    "            'observable_features': obs_batch,\n",
    "            'noise_factor': noise_batch,\n",
    "            'noisy_exp': noisy_exp_batch,\n",
    "            'true_exp': true_exp_batch,\n",
    "            'correction': correction_batch\n",
    "        }\n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    # Setup training\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Store training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'learning_rates': [],\n",
    "        'epoch_times': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        # Use tqdm with leave=False to avoid flooding output\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False):\n",
    "            # Move data to device\n",
    "            circuit_data = batch['circuit_data'].to(device)\n",
    "            obs_features = batch['observable_features'].to(device)\n",
    "            noise_factor = batch['noise_factor'].to(device)\n",
    "            noisy_exp = batch['noisy_exp'].to(device)\n",
    "            true_exp = batch['true_exp'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            corrected_exp = model(circuit_data, obs_features, noise_factor, noisy_exp)\n",
    "            \n",
    "            # Compute loss against true expectation values\n",
    "            loss = criterion(corrected_exp, true_exp)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Move data to device\n",
    "                circuit_data = batch['circuit_data'].to(device)\n",
    "                obs_features = batch['observable_features'].to(device)\n",
    "                noise_factor = batch['noise_factor'].to(device)\n",
    "                noisy_exp = batch['noisy_exp'].to(device)\n",
    "                true_exp = batch['true_exp'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                corrected_exp = model(circuit_data, obs_features, noise_factor, noisy_exp)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(corrected_exp, true_exp)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Store training metrics\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "        history['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        # Only print every 10 epochs to avoid flooding output\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, LR: {optimizer.param_groups[0]['lr']:.6f}, Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, 'best_quantum_mitigator.pt')\n",
    "    \n",
    "    # Save final model and training history\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'history': history,\n",
    "        'final_train_loss': train_loss,\n",
    "        'final_val_loss': val_loss,\n",
    "        'best_val_loss': best_val_loss,\n",
    "    }, 'final_quantum_mitigator.pt')\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(history['learning_rates'])\n",
    "    plt.yscale('log')\n",
    "    plt.title('Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs(\"ml_mitigation_output\", exist_ok=True)\n",
    "    plt.savefig(os.path.join(\"ml_mitigation_output\", \"training_history.png\"))\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Add evaluation and visualization functions\n",
    "def evaluate_mitigator(model, test_dataset, device='cpu', output_prefix=\"ml_mitigation_output\"):\n",
    "    \"\"\"Evaluate error mitigation model performance\"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Check if model is a tuple (model, history) and extract just the model\n",
    "    if isinstance(model, tuple) and len(model) > 0:\n",
    "        model = model[0]\n",
    "    \n",
    "    # Use same collate function as in training\n",
    "    def collate_fn(batch):\n",
    "        graphs = [item['circuit_graph'] for item in batch]\n",
    "        circuit_batch = Batch.from_data_list(graphs)\n",
    "        \n",
    "        obs_batch = torch.stack([item['observable_features'] for item in batch])\n",
    "        noise_batch = torch.stack([item['noise_factor'] for item in batch])\n",
    "        noisy_exp_batch = torch.stack([item['noisy_exp'] for item in batch])\n",
    "        true_exp_batch = torch.stack([item['true_exp'] for item in batch])\n",
    "        \n",
    "        return {\n",
    "            'circuit_data': circuit_batch,\n",
    "            'observable_features': obs_batch,\n",
    "            'noise_factor': noise_batch,\n",
    "            'noisy_exp': noisy_exp_batch,\n",
    "            'true_exp': true_exp_batch\n",
    "        }\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    noisy_values = []\n",
    "    corrections = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Move data to device\n",
    "            circuit_data = batch['circuit_data'].to(device)\n",
    "            obs_features = batch['observable_features'].to(device)\n",
    "            noise_factor = batch['noise_factor'].to(device)\n",
    "            noisy_exp = batch['noisy_exp'].to(device)\n",
    "            \n",
    "            # Get predictions (corrected values)\n",
    "            corrected_exp = model(circuit_data, obs_features, noise_factor, noisy_exp)\n",
    "            \n",
    "            # Calculate the applied corrections\n",
    "            applied_corrections = (corrected_exp - noisy_exp).cpu().numpy().flatten()\n",
    "            \n",
    "            # Store results\n",
    "            predictions.extend(corrected_exp.cpu().numpy().flatten())\n",
    "            true_values.extend(batch['true_exp'].numpy().flatten())\n",
    "            noisy_values.extend(batch['noisy_exp'].numpy().flatten())\n",
    "            corrections.extend(applied_corrections)\n",
    "    \n",
    "    # Calculate errors\n",
    "    noisy_error = np.abs(np.array(true_values) - np.array(noisy_values))\n",
    "    mitigated_error = np.abs(np.array(true_values) - np.array(predictions))\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nError Mitigation Results:\")\n",
    "    print(f\"  Mean noisy error: {np.mean(noisy_error):.6f}\")\n",
    "    print(f\"  Mean mitigated error: {np.mean(mitigated_error):.6f}\")\n",
    "    print(f\"  Error reduction: {(1 - np.mean(mitigated_error)/np.mean(noisy_error))*100:.2f}%\")\n",
    "    print(f\"  Mean correction applied: {np.mean(np.abs(corrections)):.6f}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot 1: Compare mitigated vs true values\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(noisy_values, true_values, alpha=0.5, label='Noisy')\n",
    "    plt.scatter(predictions, true_values, alpha=0.5, label='Mitigated')\n",
    "    plt.plot([min(true_values), max(true_values)], \n",
    "            [min(true_values), max(true_values)], 'k--')\n",
    "    plt.xlabel(\"Expectation Value\")\n",
    "    plt.ylabel(\"True Expectation Value\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Mitigation Performance\")\n",
    "    \n",
    "    # Plot 2: Error distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(noisy_error, alpha=0.5, bins=20, label='Noisy Error')\n",
    "    plt.hist(mitigated_error, alpha=0.5, bins=20, label='Mitigated Error')\n",
    "    plt.xlabel(\"Absolute Error\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Error Distribution\")\n",
    "    \n",
    "    # Plot 3: Corrections applied\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(corrections, bins=20)\n",
    "    plt.axvline(x=0, color='r', linestyle='--')\n",
    "    plt.xlabel(\"Correction Applied\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distribution of Applied Corrections\")\n",
    "    \n",
    "    # Plot 4: Correction vs Error\n",
    "    plt.subplot(2, 2, 4)\n",
    "    ideal_corrections = np.array(true_values) - np.array(noisy_values)\n",
    "    plt.scatter(ideal_corrections, corrections, alpha=0.5)\n",
    "    plt.plot([min(ideal_corrections), max(ideal_corrections)], \n",
    "             [min(ideal_corrections), max(ideal_corrections)], 'k--')\n",
    "    plt.xlabel(\"Ideal Correction\")\n",
    "    plt.ylabel(\"Applied Correction\")\n",
    "    plt.title(\"Correction Accuracy\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs(output_prefix, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_prefix, \"mitigation_results.png\"))\n",
    "    \n",
    "    # Save evaluation results - convert numpy arrays to standard Python types to ensure JSON serialization\n",
    "    evaluation_results = {\n",
    "        'predictions': [float(p) for p in predictions],\n",
    "        'true_values': [float(v) for v in true_values],\n",
    "        'noisy_values': [float(v) for v in noisy_values],\n",
    "        'corrections': [float(c) for c in corrections],\n",
    "        'noisy_error': [float(e) for e in noisy_error],\n",
    "        'mitigated_error': [float(e) for e in mitigated_error],\n",
    "        'mean_noisy_error': float(np.mean(noisy_error)),\n",
    "        'mean_mitigated_error': float(np.mean(mitigated_error)),\n",
    "        'mean_correction': float(np.mean(np.abs(corrections))),\n",
    "        'error_reduction_percentage': float((1 - np.mean(mitigated_error)/np.mean(noisy_error))*100)\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(os.path.join(output_prefix, \"evaluation_results.json\"), 'w') as f:\n",
    "        json.dump(evaluation_results, f, indent=2)\n",
    "    \n",
    "    return predictions, true_values, noisy_values\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from qiskit_aer.primitives import Estimator as AerEstimator\n",
    "\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit.library import EfficientSU2\n",
    "from qiskit.circuit.random import random_circuit\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit_aer.primitives import Estimator as AerEstimator\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "def generate_proper_circuits(\n",
    "    num_circuits: int = 500,\n",
    "    min_qubits: int = 2,\n",
    "    max_qubits: int = 6,\n",
    "    min_depth: int = 50,\n",
    "    max_depth: int = 200,\n",
    "    seed: int = 42,\n",
    "    circuit_types: list = ['random', 'su2']\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate proper quantum circuits with actual gates.\n",
    "    \n",
    "    Args:\n",
    "        num_circuits: Number of circuits to generate\n",
    "        min_qubits: Minimum number of qubits\n",
    "        max_qubits: Maximum number of qubits\n",
    "        min_depth: Minimum circuit depth\n",
    "        max_depth: Maximum circuit depth\n",
    "        seed: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        List[QuantumCircuit]: Generated circuits\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    circuits = []\n",
    "    \n",
    "    # Generate random circuits\n",
    "    if 'random' in circuit_types:\n",
    "        for i in range(num_circuits):\n",
    "            # Random number of qubits and depth\n",
    "            num_qubits = np.random.randint(min_qubits, max_qubits + 1)\n",
    "            depth = np.random.randint(min_depth, max_depth + 1)\n",
    "            \n",
    "            # Generate random circuit\n",
    "            circuit = random_circuit(\n",
    "                num_qubits=num_qubits,\n",
    "                depth=depth,\n",
    "                max_operands=2,\n",
    "                measure=False,\n",
    "                seed=seed + i\n",
    "            )\n",
    "            \n",
    "            circuit.name = f\"random_circuit_{i}\"\n",
    "            circuits.append(circuit)\n",
    "    \n",
    "    # Generate EfficientSU2 circuits (similar to hardware-efficient ansatz)\n",
    "    if 'su2' in circuit_types:\n",
    "        for i in range(num_circuits):\n",
    "            # Random number of qubits and depth\n",
    "            num_qubits = np.random.randint(min_qubits, max_qubits + 1)\n",
    "            reps = np.random.randint(1, 5)  # Number of repetitions\n",
    "            \n",
    "            # Generate EfficientSU2 circuit template\n",
    "            circuit_template = EfficientSU2(\n",
    "                num_qubits=num_qubits,\n",
    "                reps=reps,\n",
    "                entanglement='linear'\n",
    "            )\n",
    "            \n",
    "            # Convert to QuantumCircuit\n",
    "            circuit = circuit_template.decompose()\n",
    "            \n",
    "            # Add random parameters\n",
    "            params = np.random.uniform(0, 2*np.pi, circuit.num_parameters)\n",
    "            bound_circuit = circuit.assign_parameters(params)\n",
    "            \n",
    "            bound_circuit.name = f\"efficient_su2_{i}\"\n",
    "            circuits.append(bound_circuit)\n",
    "    \n",
    "    # Generate UCCSD-like circuits (with CNOT ladders and rotations)\n",
    "    if 'uccsd' in circuit_types:\n",
    "        for i in range(num_circuits):\n",
    "            # Random number of qubits\n",
    "            num_qubits = np.random.randint(min_qubits, max_qubits + 1)\n",
    "            \n",
    "            # Create circuit\n",
    "            circuit = QuantumCircuit(num_qubits)\n",
    "            \n",
    "            # Add initial Hadamard layer\n",
    "            for q in range(num_qubits):\n",
    "                circuit.h(q)\n",
    "            \n",
    "            depth = np.random.randint(min_depth, max_depth)\n",
    "            for i in range(depth):\n",
    "                start_q = np.random.randint(0, num_qubits - 1)\n",
    "                end_q = np.random.randint(start_q + 1, num_qubits)\n",
    "                if end_q == start_q: # ensure at least one CX\n",
    "                    if start_q < num_qubits -1: end_q = start_q + 1\n",
    "                    else: continue # skip if not possible\n",
    "\n",
    "                # Forward ladder on subset\n",
    "                for q in range(start_q, end_q):\n",
    "                    circuit.cx(q, q + 1)\n",
    "\n",
    "                # Rotations (maybe only on involved qubits or all)\n",
    "                for q in range(num_qubits): # Or range(start_q, end_q + 1)\n",
    "                    angle = np.random.uniform(0, 2*np.pi)\n",
    "                    circuit.rz(angle, q)\n",
    "\n",
    "                # Backward ladder on subset\n",
    "                for q in range(end_q - 1, start_q - 1, -1):\n",
    "                    circuit.cx(q, q + 1) # Still cx(q, q+1) but iterating downwards\n",
    "                                        # or to be truly \"backward\": circuit.cx(q, q-1) if q > start_q\n",
    "                                        # For true reversal of CX(q, q+1) it's more complex, often involving H gates\n",
    "                                        # or simply reversing the order of application:\n",
    "                # Backward ladder (applied in reverse order of forward)\n",
    "                for q in range(end_q -1, start_q -1, -1): # Iterate from end_q-1 down to start_q\n",
    "                    circuit.cx(q, q + 1)\n",
    "            \n",
    "            circuit.name = f\"uccsd_like_{i}\"\n",
    "            circuits.append(circuit)\n",
    "    \n",
    "    return circuits\n",
    "\n",
    "def generate_proper_observables(circuits, max_terms=8, seed=42):\n",
    "    \"\"\"\n",
    "    Generate proper observables for the circuits.\n",
    "    \n",
    "    Args:\n",
    "        circuits: List of quantum circuits\n",
    "        max_terms: Maximum number of terms in each observable\n",
    "        seed: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        List[SparsePauliOp]: Generated observables\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    observables = []\n",
    "    \n",
    "    for circuit in circuits:\n",
    "        num_qubits = circuit.num_qubits\n",
    "        num_terms = np.random.randint(1, max_terms + 1)\n",
    "        \n",
    "        # Generate Pauli strings\n",
    "        pauli_strings = []\n",
    "        coeffs = []\n",
    "        \n",
    "        for _ in range(num_terms):\n",
    "            # Generate a random Pauli string\n",
    "            pauli_ops = np.random.choice(['I', 'X', 'Y', 'Z'], size=num_qubits)\n",
    "            pauli_string = ''.join(pauli_ops)\n",
    "            \n",
    "            # Generate a random coefficient\n",
    "            coeff = np.random.uniform(-1, 1)\n",
    "            \n",
    "            pauli_strings.append(pauli_string)\n",
    "            coeffs.append(coeff)\n",
    "        \n",
    "        # Create observable\n",
    "        observable = SparsePauliOp(pauli_strings, coeffs)\n",
    "        observables.append(observable)\n",
    "    \n",
    "    return observables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for data generation\n",
    "num_circuits = 800\n",
    "circuit_types = ['random'] # su(2) do for more structured circuits next\n",
    "\n",
    "\n",
    "min_qubits = 2\n",
    "max_qubits = 8\n",
    "min_depth = 100\n",
    "max_depth = 500\n",
    "shots = 8192\n",
    "noise_factor = 0.2\n",
    "output_dir = \"ml_mitigation_output\"\n",
    "seed = 42\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "from qiskit.converters import circuit_to_dag\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. First, add circuit-to-graph conversion function\n",
    "def circuit_to_graph_data(circuit):\n",
    "    \"\"\"Convert a Qiskit circuit to a PyTorch Geometric graph\"\"\"\n",
    "    dag = circuit_to_dag(circuit)\n",
    "    \n",
    "    # Map operations to indices\n",
    "    op_types = {'x', 'h', 'cx', 'rz', 'ry', 'rx', 'p', 'u', 'u1', 'u2', 'u3', 'id', 'barrier', 'measure'}\n",
    "    op_type_map = {op: i for i, op in enumerate(sorted(op_types))}\n",
    "    \n",
    "    # Create node features\n",
    "    node_features = []\n",
    "    node_map = {}\n",
    "    \n",
    "    # Get all nodes (proper method)\n",
    "    op_nodes = list(dag.op_nodes())\n",
    "    in_nodes = list(dag.input_map.values())\n",
    "    out_nodes = list(dag.output_map.values())\n",
    "    all_nodes = op_nodes + in_nodes + out_nodes\n",
    "    \n",
    "    # Create features for each node\n",
    "    for i, node in enumerate(all_nodes):\n",
    "        node_map[node] = i\n",
    "        feature = torch.zeros(len(op_types) + 3)\n",
    "        \n",
    "        if node in op_nodes:  # Operation node\n",
    "            op_name = node.op.name if hasattr(node.op, 'name') else str(node.op)\n",
    "            op_idx = op_type_map.get(op_name.lower(), len(op_types) - 1)\n",
    "            feature[op_idx] = 1.0\n",
    "            \n",
    "            # Add qubit position if available\n",
    "            if node.qargs and len(node.qargs) > 0:\n",
    "                try:\n",
    "                    qubit_idx = circuit.find_bit(node.qargs[0]).index\n",
    "                    feature[-1] = qubit_idx / max(1, circuit.num_qubits - 1)\n",
    "                except:\n",
    "                    feature[-1] = 0\n",
    "        elif node in in_nodes:  # Input node\n",
    "            feature[len(op_types)] = 1.0\n",
    "        elif node in out_nodes:  # Output node\n",
    "            feature[len(op_types) + 1] = 1.0\n",
    "        \n",
    "        node_features.append(feature)\n",
    "    \n",
    "    # Stack features into tensor\n",
    "    node_features = torch.stack(node_features)\n",
    "    \n",
    "    # Create edge index by traversing the DAG\n",
    "    edge_index = []\n",
    "    \n",
    "    # For each node, add edges to all its successors\n",
    "    for node in all_nodes:\n",
    "        if node in op_nodes or node in in_nodes:  # Don't get successors of output nodes\n",
    "            successors = list(dag.successors(node))\n",
    "            for succ in successors:\n",
    "                if node in node_map and succ in node_map:\n",
    "                    edge_index.append([node_map[node], node_map[succ]])\n",
    "    \n",
    "    # Convert to tensor\n",
    "    if edge_index:\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    else:\n",
    "        # Empty edge index fallback\n",
    "        edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "    \n",
    "    return Data(x=node_features, edge_index=edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate proper circuits\n",
    "circuits = generate_proper_circuits(\n",
    "    num_circuits=num_circuits,\n",
    "    min_qubits=min_qubits,\n",
    "    max_qubits=max_qubits,\n",
    "    min_depth=min_depth,\n",
    "    max_depth=max_depth,\n",
    "    seed=seed,\n",
    "    circuit_types=circuit_types\n",
    ")\n",
    "\n",
    "observables = generate_proper_observables(circuits, seed=seed)\n",
    "\n",
    "# Create a combined dataset with multiple noise factors\n",
    "# noise_factors = [1.0, 1.2, 1.4]\n",
    "noise_factors = [1.0]\n",
    "\n",
    "all_circuits_by_nf = []\n",
    "all_observables_by_nf = []\n",
    "all_noisy_values_by_nf = []\n",
    "all_noiseless_values_by_nf = []\n",
    "\n",
    "# Run simulations for each noise factor\n",
    "for noise_factor in noise_factors:\n",
    "    output_dir = f\"ml_mitigation_output/noise_factor_{noise_factor}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"\\nProcessing noise factor: {noise_factor}\")\n",
    "    \n",
    "    noise_model = get_quera_noise_model(noise_factor)\n",
    "    \n",
    "    noiseless_estimator_options = {\n",
    "        \"run_options\": {\"seed\": seed, \"shots\": None},\n",
    "        \"backend_options\": {\n",
    "            \"method\": \"statevector\",\n",
    "            \"enable_truncation\": True,\n",
    "            \"device\": \"CPU\",\n",
    "            \"noise_model\": None,\n",
    "            \"shots\": None,\n",
    "        },\n",
    "        \"transpile_options\": {\"seed_transpiler\": seed},\n",
    "        \"approximation\": True,\n",
    "    }\n",
    "    \n",
    "    noisy_estimator_options = {\n",
    "        \"run_options\": {\"seed\": seed, \"shots\": None},\n",
    "        \"backend_options\": {\n",
    "            \"method\": \"density_matrix\",\n",
    "            \"enable_truncation\": True,\n",
    "            \"device\": \"CPU\",\n",
    "            \"noise_model\": noise_model,\n",
    "            \"shots\": None,\n",
    "        },\n",
    "        \"transpile_options\": {\"seed_transpiler\": seed},\n",
    "        \"approximation\": True,\n",
    "    }\n",
    "    \n",
    "    # Create estimators\n",
    "    noiseless_estimator = AerEstimator(**noiseless_estimator_options)\n",
    "    noisy_estimator = AerEstimator(**noisy_estimator_options)\n",
    "    \n",
    "    # Prepare parameter values (empty for non-parameterized circuits)\n",
    "    parameter_values = [[] for _ in range(len(circuits))]\n",
    "    \n",
    "    # Run noiseless simulations\n",
    "    print(\"Running noiseless simulations...\")\n",
    "    noiseless_job = noiseless_estimator.run(\n",
    "        circuits=circuits,\n",
    "        observables=observables,\n",
    "        parameter_values=parameter_values\n",
    "    )\n",
    "    noiseless_result = noiseless_job.result()\n",
    "    noiseless_values = noiseless_result.values\n",
    "    \n",
    "    # Run noisy simulations\n",
    "    print(\"Running noisy simulations...\")\n",
    "    noisy_job = noisy_estimator.run(\n",
    "        circuits=circuits,\n",
    "        observables=observables,\n",
    "        parameter_values=parameter_values\n",
    "    )\n",
    "    noisy_result = noisy_job.result()\n",
    "    noisy_values = noisy_result.values\n",
    "    \n",
    "    # Store data for this noise factor\n",
    "    curr_noisy_values = [float(val) for val in noisy_values]\n",
    "    curr_noiseless_values = [float(val) for val in noiseless_values]\n",
    "    \n",
    "    all_circuits_by_nf.append(circuits)\n",
    "    all_observables_by_nf.append(observables)\n",
    "    all_noisy_values_by_nf.append(curr_noisy_values)\n",
    "    all_noiseless_values_by_nf.append(curr_noiseless_values)\n",
    "    \n",
    "    # Print summary statistics for this noise factor\n",
    "    error = np.abs(np.array(noiseless_values) - np.array(noisy_values))\n",
    "    print(f\"  Noise factor {noise_factor} statistics:\")\n",
    "    print(f\"    Mean absolute error: {np.mean(error):.6f}\")\n",
    "    print(f\"    Max absolute error: {np.max(error):.6f}\")\n",
    "    print(f\"    Min absolute error: {np.min(error):.6f}\")\n",
    "    \n",
    "    # Create visualization for this noise factor\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter([float(v) for v in noisy_values], [float(v) for v in noiseless_values], alpha=0.5)\n",
    "    plt.plot([min([float(v) for v in noiseless_values]), max([float(v) for v in noiseless_values])], \n",
    "             [min([float(v) for v in noiseless_values]), max([float(v) for v in noiseless_values])], 'k--')\n",
    "    plt.xlabel(\"Noisy Expectation Value\")\n",
    "    plt.ylabel(\"Noiseless Expectation Value\")\n",
    "    plt.title(f\"Noisy vs Noiseless Expectation Values (Noise Factor: {noise_factor})\")\n",
    "    plt.savefig(os.path.join(output_dir, f\"expectation_values_nf_{noise_factor}.png\"))\n",
    "    print(f\"Saved visualization to {os.path.join(output_dir, f'expectation_values_nf_{noise_factor}.png')}\")\n",
    "\n",
    "# Print overall summary statistics\n",
    "all_noisy_flat = [val for sublist in all_noisy_values_by_nf for val in sublist]\n",
    "all_noiseless_flat = [val for sublist in all_noiseless_values_by_nf for val in sublist]\n",
    "error = np.abs(np.array(all_noiseless_flat) - np.array(all_noisy_flat))\n",
    "\n",
    "print(\"\\nOverall summary statistics:\")\n",
    "print(f\"  Total data points: {len(all_noisy_flat)}\")\n",
    "print(f\"  Mean absolute error: {np.mean(error):.6f}\")\n",
    "print(f\"  Max absolute error: {np.max(error):.6f}\")\n",
    "print(f\"  Min absolute error: {np.min(error):.6f}\")\n",
    "print(f\"  Standard deviation of error: {np.std(error):.6f}\")\n",
    "\n",
    "# Create combined visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, nf in enumerate(noise_factors):\n",
    "    plt.scatter(\n",
    "        all_noisy_values_by_nf[i],\n",
    "        all_noiseless_values_by_nf[i],\n",
    "        alpha=0.5,\n",
    "        label=f\"Noise Factor: {nf}\"\n",
    "    )\n",
    "plt.plot([min(all_noiseless_flat), max(all_noiseless_flat)], \n",
    "         [min(all_noiseless_flat), max(all_noiseless_flat)], 'k--')\n",
    "plt.xlabel(\"Noisy Expectation Value\")\n",
    "plt.ylabel(\"Noiseless Expectation Value\")\n",
    "plt.title(\"Noisy vs Noiseless Expectation Values (Multiple Noise Factors)\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(output_dir, \"expectation_values_combined.png\"))\n",
    "print(f\"Saved combined visualization to {os.path.join(output_dir, 'expectation_values_combined.png')}\")\n",
    "\n",
    "# Convert your data to the format needed for the DAG-based model\n",
    "print(\"Preparing dataset for DAG-based error mitigation model...\")\n",
    "dataset = prepare_mitigator_dataset(\n",
    "    all_circuits_by_nf,  # List of lists of circuits for each noise factor\n",
    "    all_observables_by_nf,  # List of lists of observables for each noise factor\n",
    "    all_noisy_values_by_nf,  # List of lists of noisy values for each noise factor\n",
    "    all_noiseless_values_by_nf,  # List of lists of noiseless values for each noise factor\n",
    "    noise_factors  # List of noise factors\n",
    ")\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "print(f\"Dataset split: {len(train_dataset)} training samples, {len(test_dataset)} test samples\")\n",
    "\n",
    "\n",
    "from qiskit import qasm3\n",
    "import hashlib\n",
    "def qasm_hash(circ: QuantumCircuit) -> str:\n",
    "    \"\"\"Creates a stable hash from a circuit's QASM3 representation.\"\"\"\n",
    "    try:\n",
    "        qasm = qasm3.dumps(circ)\n",
    "        return hashlib.sha256(qasm.encode()).hexdigest()\n",
    "    except Exception:\n",
    "        # Fallback for circuits that might have issues with QASM3 conversion\n",
    "        return hashlib.sha256(str(circ.data).encode()).hexdigest()\n",
    "    \n",
    "train_hashes = []\n",
    "for data in train_dataset:\n",
    "    hash = qasm_hash(data['circuit'])\n",
    "    train_hashes.append(hash)\n",
    "\n",
    "test_hashes = []\n",
    "for data in test_dataset:\n",
    "    hash = qasm_hash(data['circuit'])\n",
    "    test_hashes.append(hash)\n",
    "\n",
    "# Determine the number of hashes that are shared between train and test sets\n",
    "train_hashes_set = set(train_hashes)\n",
    "test_hashes_set = set(test_hashes)\n",
    "shared_hashes = train_hashes_set.intersection(test_hashes_set)\n",
    "print(f\"Number of shared circuit hashes between training and test sets: {len(shared_hashes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu') # For Apple Silicon\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create and train the model\n",
    "model = QErrorMitigationModel(depth=4)\n",
    "print(\"Training error mitigation model...\")\n",
    "trained_model, history = train_mitigator(\n",
    "    train_dataset, \n",
    "    model, \n",
    "    epochs=100,  # Adjust based on your needs\n",
    "    batch_size=16,\n",
    "    lr=0.001,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Evaluate model performance on test data\n",
    "print(\"Evaluating model performance on test data...\")\n",
    "predictions, true_values, noisy_values = evaluate_mitigator(trained_model, test_dataset, device=device, output_prefix=output_dir)\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': trained_model.state_dict(),\n",
    "    'architecture': 'QErrorMitigationModel',\n",
    "    'creation_date': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'noise_factors': noise_factors\n",
    "}, os.path.join(output_dir, \"quantum_error_mitigator.pt\"))\n",
    "print(f\"Saved model to {os.path.join(output_dir, 'quantum_error_mitigator.pt')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Evaluate model on different noise factors for interpolation and extrapolation analysis\n",
    "noise_factors = [0.1, 0.3, 0.5, 0.7, 0.9]  # Lower, in-between, and higher than training values\n",
    "results = []\n",
    "\n",
    "for noise_factor in noise_factors:\n",
    "    print(f\"\\nEvaluating model on noise_factor={noise_factor}...\")\n",
    "\n",
    "    output_dir = f\"ml_mitigation_output/noise_factor_{noise_factor}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate proper circuits\n",
    "    circuits = generate_proper_circuits(\n",
    "        num_circuits=num_circuits,\n",
    "        min_qubits=min_qubits,\n",
    "        max_qubits=max_qubits,\n",
    "        min_depth=min_depth,\n",
    "        max_depth=max_depth,\n",
    "        seed=seed,\n",
    "        circuit_types=circuit_types\n",
    "    )\n",
    "\n",
    "    observables = generate_proper_observables(circuits, seed=seed)\n",
    "\n",
    "    noise_model = get_quera_noise_model(noise_factor)\n",
    "\n",
    "    noiseless_estimator_options = {\n",
    "        \"run_options\": {\"seed\": seed, \"shots\": None},\n",
    "        \"backend_options\": {\n",
    "            \"method\": \"statevector\",\n",
    "            \"enable_truncation\": True,\n",
    "            \"device\": \"CPU\",\n",
    "            \"noise_model\": None,\n",
    "            \"shots\": None,\n",
    "        },\n",
    "        \"transpile_options\": {\"seed_transpiler\": seed},\n",
    "        \"approximation\": True,\n",
    "    }\n",
    "\n",
    "    noisy_estimator_options = {\n",
    "        \"run_options\": {\"seed\": seed, \"shots\": shots},\n",
    "        \"backend_options\": {\n",
    "            \"method\": \"density_matrix\",\n",
    "            \"enable_truncation\": True,\n",
    "            \"device\": \"CPU\",\n",
    "            \"noise_model\": noise_model,\n",
    "            \"shots\": shots,\n",
    "        },\n",
    "        \"transpile_options\": {\"seed_transpiler\": seed},\n",
    "        \"approximation\": True,\n",
    "    }\n",
    "\n",
    "    # Create estimators\n",
    "    noiseless_estimator = AerEstimator(**noiseless_estimator_options)\n",
    "    noisy_estimator = AerEstimator(**noisy_estimator_options)\n",
    "\n",
    "    # Prepare parameter values (empty for non-parameterized circuits)\n",
    "    parameter_values = [[] for _ in range(len(circuits))]\n",
    "\n",
    "    # Run noiseless simulations\n",
    "    print(\"Running noiseless simulations...\")\n",
    "    noiseless_job = noiseless_estimator.run(\n",
    "        circuits=circuits,\n",
    "        observables=observables,\n",
    "        parameter_values=parameter_values\n",
    "    )\n",
    "    noiseless_result = noiseless_job.result()\n",
    "    noiseless_values = noiseless_result.values\n",
    "\n",
    "    # Run noisy simulations\n",
    "    print(\"Running noisy simulations...\")\n",
    "    noisy_job = noisy_estimator.run(\n",
    "        circuits=circuits,\n",
    "        observables=observables,\n",
    "        parameter_values=parameter_values\n",
    "    )\n",
    "    noisy_result = noisy_job.result()\n",
    "    noisy_values = noisy_result.values\n",
    "\n",
    "    noiseless_val_list = []\n",
    "    noisy_val_list = []\n",
    "\n",
    "    for i, (circuit, observable, noisy_val, noiseless_val) in enumerate(\n",
    "        zip(circuits, observables, noisy_values, noiseless_values)\n",
    "    ):\n",
    "        noiseless_val_list.append(float(noiseless_val))\n",
    "        noisy_val_list.append(float(noisy_val))\n",
    "\n",
    "    # Print summary statistics\n",
    "    error = np.abs(np.array(noiseless_val_list) - np.array(noisy_val_list))\n",
    "    \n",
    "    print(f\"Summary statistics for noise_factor={noise_factor}:\")\n",
    "    print(f\"  Mean absolute error: {np.mean(error):.6f}\")\n",
    "    print(f\"  Max absolute error: {np.max(error):.6f}\")\n",
    "    print(f\"  Min absolute error: {np.min(error):.6f}\")\n",
    "    print(f\"  Standard deviation of error: {np.std(error):.6f}\")\n",
    "\n",
    "    # Create visualization for this noise factor\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(noisy_val_list, noiseless_val_list, alpha=0.5)\n",
    "    plt.plot([min(noiseless_val_list), max(noiseless_val_list)], \n",
    "              [min(noiseless_val_list), max(noiseless_val_list)], 'k--')\n",
    "    plt.xlabel(\"Noisy Expectation Value\")\n",
    "    plt.ylabel(\"Noiseless Expectation Value\")\n",
    "    plt.title(f\"Noisy vs Noiseless Expectation Values (Noise Factor: {noise_factor})\")\n",
    "    plt.savefig(os.path.join(output_dir, f\"expectation_values_nf_{noise_factor}.png\"))\n",
    "    \n",
    "    # Convert data to the format needed for the DAG-based model\n",
    "    print(f\"Preparing dataset for noise_factor={noise_factor}...\")\n",
    "    dataset = prepare_mitigator_dataset(\n",
    "        circuits, \n",
    "        observables, \n",
    "        noisy_val_list, \n",
    "        noiseless_val_list, \n",
    "        noise_factor\n",
    "    )\n",
    "\n",
    "    # Evaluate model on this dataset\n",
    "    print(f\"Evaluating model on noise_factor={noise_factor}...\")\n",
    "    predictions, true_values, noisy_values = evaluate_mitigator(trained_model, dataset, device=device, output_prefix=output_dir)\n",
    "    \n",
    "    # Calculate errors\n",
    "    noisy_error = np.abs(np.array(true_values) - np.array(noisy_values))\n",
    "    mitigated_error = np.abs(np.array(true_values) - np.array(predictions))\n",
    "    error_reduction = (1 - np.mean(mitigated_error)/np.mean(noisy_error))*100\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'noise_factor': noise_factor,\n",
    "        'noisy_error': np.mean(noisy_error),\n",
    "        'mitigated_error': np.mean(mitigated_error),\n",
    "        'error_reduction': error_reduction,\n",
    "        'is_interpolation': 0.2 <= noise_factor <= 0.6  # True if within training range\n",
    "    })\n",
    "\n",
    "# Create summary visualization for interpolation/extrapolation analysis\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Error vs Noise Factor\n",
    "plt.subplot(2, 1, 1)\n",
    "noise_factors = [r['noise_factor'] for r in results]\n",
    "noisy_errors = [r['noisy_error'] for r in results]\n",
    "mitigated_errors = [r['mitigated_error'] for r in results]\n",
    "\n",
    "# Highlight training region\n",
    "plt.axvspan(0.2, 0.6, alpha=0.2, color='green', label='Training Range')\n",
    "\n",
    "plt.plot(noise_factors, noisy_errors, 'o-', label='Noisy Error')\n",
    "plt.plot(noise_factors, mitigated_errors, 'o-', label='Mitigated Error')\n",
    "plt.xlabel('Noise Factor')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Error vs Noise Factor (Interpolation and Extrapolation)')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot 2: Error Reduction vs Noise Factor\n",
    "plt.subplot(2, 1, 2)\n",
    "error_reductions = [r['error_reduction'] for r in results]\n",
    "\n",
    "# Highlight training region\n",
    "plt.axvspan(0.2, 0.6, alpha=0.2, color='green', label='Training Range')\n",
    "\n",
    "plt.plot(noise_factors, error_reductions, 'o-', color='purple')\n",
    "plt.xlabel('Noise Factor')\n",
    "plt.ylabel('Error Reduction (%)')\n",
    "plt.title('Error Reduction vs Noise Factor')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"interpolation_extrapolation_analysis.png\"))\n",
    "print(f\"Saved interpolation/extrapolation analysis to {os.path.join(output_dir, 'interpolation_extrapolation_analysis.png')}\")\n",
    "\n",
    "\n",
    "# Plot average correction value vs noise factor\n",
    "print(\"Plotting average correction value vs noise factor...\")\n",
    "avg_corrections = []\n",
    "\n",
    "for noise_factor in noise_factors:\n",
    "    output_dir = f\"ml_mitigation_output/noise_factor_{noise_factor}\"\n",
    "    eval_results_path = os.path.join(output_dir, \"evaluation_results.json\")\n",
    "    \n",
    "    try:\n",
    "        with open(eval_results_path, 'r') as f:\n",
    "            eval_results = json.load(f)\n",
    "        \n",
    "        corrections = eval_results.get(\"corrections\", [])\n",
    "        if corrections:\n",
    "            avg_correction = np.mean(np.abs(corrections))\n",
    "            avg_corrections.append(avg_correction)\n",
    "            print(f\"  Noise factor {noise_factor}: Average correction = {avg_correction:.6f}\")\n",
    "        else:\n",
    "            print(f\"  No corrections found for noise factor {noise_factor}\")\n",
    "            avg_corrections.append(0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Evaluation results file not found for noise factor {noise_factor}\")\n",
    "        avg_corrections.append(0)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(noise_factors, avg_corrections, 'o-', linewidth=2, markersize=10)\n",
    "plt.xlabel(\"Noise Factor\", fontsize=14)\n",
    "plt.ylabel(\"Average Correction Value\", fontsize=14)\n",
    "plt.title(\"Average Correction Value vs Noise Factor\", fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add data points labels\n",
    "for i, (x, y) in enumerate(zip(noise_factors, avg_corrections)):\n",
    "    plt.annotate(f\"{y:.4f}\", (x, y), textcoords=\"offset points\", \n",
    "                 xytext=(0, 10), ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(\"ml_mitigation_output\", \"avg_correction_vs_noise_factor.png\"))\n",
    "print(f\"Saved average correction plot to ml_mitigation_output/avg_correction_vs_noise_factor.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# Create a dummy input for each component\n",
    "batch_size = 1\n",
    "dummy_x = torch.randn(10, 17)  # 10 nodes with 17 features\n",
    "dummy_edge_index = torch.tensor([[0, 1, 2, 3], [1, 2, 3, 0]]) # Example edges\n",
    "dummy_batch = torch.zeros(10, dtype=torch.long)  # All nodes in same graph\n",
    "\n",
    "dummy_circuit_data = Data(x=dummy_x, edge_index=dummy_edge_index, batch=dummy_batch)\n",
    "dummy_observable = torch.randn(batch_size, 5)\n",
    "dummy_noise = torch.randn(batch_size, 1)\n",
    "dummy_noisy_exp = torch.randn(batch_size, 1)\n",
    "\n",
    "model = QErrorMitigationModel()\n",
    "output = model(dummy_circuit_data, dummy_observable, dummy_noise, dummy_noisy_exp)\n",
    "\n",
    "# Generate the graph\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.render(\"model_architecture\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for each component with simplified labels\n",
    "modules = {\n",
    "    # Main components\n",
    "    \"circuit_input\": \"Circuit Input\",\n",
    "    \"observable_input\": \"Observable Input\",\n",
    "    \"noise_input\": \"Noise Factor\",\n",
    "    \"noisy_exp\": \"Noisy Expectation\",\n",
    "    \n",
    "    # Circuit path\n",
    "    \"gat1\": \"Graph Attention\",\n",
    "    \"circuit_emb\": \"Circuit Embedding\",\n",
    "    \n",
    "    # Observable path\n",
    "    \"obs_lin1\": \"MLP\",\n",
    "    \"obs_emb\": \"Observable Embedding\",\n",
    "    \n",
    "    # Noise path\n",
    "    \"noise_lin\": \"MLP\",\n",
    "    \"noise_emb\": \"Noise Embedding\",\n",
    "    \n",
    "    # Fusion network\n",
    "    \"concat\": \"Concatenate\",\n",
    "    \"fusion1\": \"MLP\",\n",
    "    \"dropout\": \"Dropout\",\n",
    "    \"fusion2\": \"MLP\",\n",
    "    \"output\": \"Corrected Expectation\"\n",
    "}\n",
    "\n",
    "# Add nodes with labels\n",
    "for name, label in modules.items():\n",
    "    G.add_node(name, label=label)\n",
    "\n",
    "# Add edges\n",
    "edges = [\n",
    "    # Circuit path\n",
    "    (\"circuit_input\", \"gat1\"), (\"gat1\", \"circuit_emb\"), (\"circuit_emb\", \"concat\"),\n",
    "    \n",
    "    # Observable path\n",
    "    (\"observable_input\", \"obs_lin1\"),\n",
    "    (\"obs_lin1\", \"obs_emb\"), (\"obs_emb\", \"concat\"),\n",
    "    \n",
    "    # Noise path\n",
    "    (\"noise_input\", \"noise_lin\"), (\"noise_lin\", \"noise_emb\"), (\"noise_emb\", \"concat\"),\n",
    "    \n",
    "    # Noisy expectation\n",
    "    (\"noisy_exp\", \"concat\"),\n",
    "    \n",
    "    # Fusion path\n",
    "    (\"concat\", \"fusion1\"), (\"fusion1\", \"dropout\"), (\"dropout\", \"fusion2\"),  (\"fusion2\", \"output\")\n",
    "]\n",
    "\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(20, 14))  # Increased figure size\n",
    "# Use a more compact layout\n",
    "pos = nx.nx_pydot.pydot_layout(G, prog=\"dot\")\n",
    "\n",
    "# Define node colors by group\n",
    "node_colors = {\n",
    "    'circuit': '#D6EAF8',  # Circuit: light blue\n",
    "    'obs': '#D5F5E3',      # Observable: light green\n",
    "    'noise': '#FADBD8',    # Noise: light red\n",
    "    'noisy': '#F9E79F',    # Yellow\n",
    "    'fusion': '#EAF2F8',   # Light blue\n",
    "    'other': '#FFFFFF'     # White\n",
    "}\n",
    "\n",
    "colors = []\n",
    "for node in G.nodes():\n",
    "    if any(x in node for x in ['circuit', 'gat', 'global_pool']):\n",
    "        colors.append(node_colors['circuit'])\n",
    "    elif 'obs' in node:\n",
    "        colors.append(node_colors['obs'])\n",
    "    elif 'noise' in node:\n",
    "        colors.append(node_colors['noise'])\n",
    "    elif 'noisy_exp' in node:\n",
    "        colors.append(node_colors['noisy'])\n",
    "    elif any(x in node for x in ['fusion', 'dropout', 'concat']):\n",
    "        colors.append(node_colors['fusion'])\n",
    "    else:\n",
    "        colors.append(node_colors['other'])\n",
    "\n",
    "# Draw nodes and edges with larger node size\n",
    "nx.draw_networkx_nodes(G, pos, node_size=3000, node_color=colors, alpha=0.8)\n",
    "nx.draw_networkx_edges(G, pos, arrows=True, arrowsize=20, width=1.5)\n",
    "\n",
    "# Add labels - use node names directly if label attribute is missing\n",
    "labels = {}\n",
    "for node in G.nodes():\n",
    "    if 'label' in G.nodes[node]:\n",
    "        labels[node] = G.nodes[node]['label']\n",
    "    else:\n",
    "        labels[node] = node  # Fallback to node name if label is missing\n",
    "\n",
    "nx.draw_networkx_labels(G, pos, labels=labels, font_size=22, font_weight='bold')\n",
    "\n",
    "plt.title(\"QErrorMitigationModel Architecture\", fontsize=32)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('qem_model_architecture.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
